{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sujet de veille n°8\n",
    "\n",
    "## Pourquoi les features corrélées posent problème dans un modèle de régression linéaire?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**sources :** \n",
    "\n",
    "http://larmarange.github.io/analyse-R/multicolinearite.html\n",
    "        \n",
    "\"Dans une régression, la **multicolinéarité** est un problème qui survient lorsque certaines variables de prévision du modèle mesurent le même phénomène. \n",
    "Une multicolinéarité prononcée s’avère problématique, car elle peut augmenter la variance des coefficients de régression et les rendre instables et difficiles à interpréter. \n",
    "Les conséquences de coefficients instables peuvent être les suivantes :\n",
    "\n",
    "- les coefficients peuvent sembler non significatifs, même lorsqu’une relation significative existe entre le prédicteur et la réponse ;\n",
    "- les coefficients de prédicteurs fortement corrélés varieront considérablement d’un échantillon à un autre ;\n",
    "- lorsque des termes d’un modèle sont fortement corrélés, la suppression de l’un de ces termes aura une incidence considérable sur les coefficients estimés des autres. Les coefficients des termes fortement corrélés peuvent même présenter le mauvais signe.\n",
    "\n",
    "La multicolinéarité n’a aucune incidence sur l’adéquation de l’ajustement, ni sur la qualité de la prévision. \n",
    "Cependant, les coefficients individuels associés à chaque variable explicative ne peuvent pas être interprétés de façon fiable.\n",
    "\n",
    "\n",
    "Une **erreur fréquente** est de confondre multicolinéarité et corrélation. Si des variables colinéaires sont de facto fortement corrélées entre elles, deux variables corrélées ne sont pas forcément colinéaires. En termes non statistiques, il y a colinéarité lorsque deux ou plusieurs variables mesurent la \"même chose\".\n",
    "\n",
    "\n",
    "Prenons un **exemple**. Nous étudions les complications après l’accouchement dans différentes maternités d’un pays en développement. On souhaite mettre dans le modèle, à la fois le milieu de résidence (urbain ou rural) et le fait qu’il y ait ou non un médecin dans la clinique. Or, dans la zone d’enquête, les maternités rurales sont dirigées seulement par des sage-femmes tandis que l’on trouve un médecin dans toutes les maternités urbaines sauf une. Dès lors, dans ce contexte précis, le milieu de résidence prédit presque totalement la présence d’un médecin et on se retrouve face à une multicolinéarité (qui serait même parfaite s’il n’y avait pas une clinique urbaine sans médecin). On ne peut donc distinguer l’effet de la présence d’un médecin de celui du milieu de résidence et il ne faut mettre qu’une seule de ces deux variables dans le modèle, sachant que du point de vue de l’interprétation elle capturera à la fois l’effet de la présence d’un médecin et celui du milieu de résidence.\n",
    "\n",
    "Par contre, si dans notre région d’étude, seule la moitié des maternités urbaines disposait d’un médecin, alors le milieu de résidence n’aurait pas été suffisant pour prédire la présence d’un médecin. Certes, les deux variables seraient corrélées mais pas colinéaires. Un autre exemple de corrélation sans colinéarité, c’est la relation entre milieu de résidence et niveau d’instruction. Il y a une corrélation entre ces deux variables, les personnes résidant en ville étant généralement plus instruites. Cependant, il existe également des personnes non instruites en ville et des personnes instruites en milieu rural. Le milieu de résidence n’est donc pas suffisant pour prédire le niveau d’instruction.\n",
    "\n",
    "\n",
    "**Mesure de la colinéarité**\n",
    "Il existe différentes mesures de la multicolinéarité. L’extension mctest en fournie plusieurs, mais elle n’est utilisable que si l’ensemble des variables explicatives sont de type numérique.\n",
    "L’approche la plus classique consiste à examiner les facteurs d’inflation de la variance (FIV) ou **variance inflation factor (VIF)** en anglais. *( voir cours de Jean-Paul)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**source :**\n",
    "\n",
    "http://eric.univ-lyon2.fr/~ricco/cours/slides/Reg_Multiple_Colinearite_Selection_Variables.pdf\n",
    "\n",
    "**Quelles conséquences ?**\n",
    "- Variances des estimateurs exagérées\n",
    "- Les t de Studentsont sous-estimés, les variables ne paraissent pas significatives (cf. cylindrée, puissance)\n",
    "- Les valeurs/signes des coefficients sont contradictoires, ne concordent pas avec les connaissances du domaine (puissance est reliée négativement avec la consommation ????)\n",
    "- Les résultats sont très instables, l’adjonction ou la suppression de quelques observations modifient fortement les valeurs et les signes des coefficients\n",
    "- -> Lecture des résultats très périlleuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**source :**\n",
    "\n",
    "https://fr.wikipedia.org/wiki/R%C3%A9gression_lin%C3%A9aire#Non_colin%C3%A9arit%C3%A9_des_variables_explicatives\n",
    "\n",
    "#### Principales hypothèses\n",
    "\n",
    "Les hypothèses de Gauss-Markov et les hypothèses de normalité garantissent des propriétés particulièrement intéressantes des estimateurs des coefficients de régression5. Les hypothèses peuvent s'exprimer différemment selon qu'il s'agisse de la régression linéaire simple ou multiple, ou bien selon que les ( x i , j ) i = 1 , . . , n ; j = 1 , . . . , K {\\displaystyle (x_{i,j})_{i=1,..,n;\\quad j=1,...,K}} {\\displaystyle (x_{i,j})_{i=1,..,n;\\quad j=1,...,K}} Note 1 sont des valeurs constantes (comme une unité de temps par exemple), ou un échantillon des valeurs d'une variable aléatoire.\n",
    "\n",
    "#### Non colinéarité des variables explicatives\n",
    "\n",
    "**Cette hypothèse suppose qu'aucune des variables explicatives du modèle ne peut s'écrire comme une combinaison linéaire des autres variables.** \n",
    "Ce qui revient à E ( x i x i ′ ) {\\displaystyle \\mathbb {E} (x_{i}x_{i}')} \\mathbb E (x_ix_i') inversible avec xi' la transposée du vecteur xi en notation vectorielle et à E ( X ′ X ) {\\displaystyle \\mathbb {E} (X'X)} \\mathbb E (X'X) inversible avec X' la transposée de la matrice X en notation matricielle. Cette condition est souvent exprimée par le fait que la matrice X est de rang maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Présentation orale : \n",
    "\n",
    "**Faire une présentation avec la formule maths de la régression linéaire**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Revenons la définition d'un modèle de régression linéaire (Jean-Paul Maalouf)\n",
    "\n",
    "\"Un modèle permet d’**expliquer une variable** (*à expliquer, dépendante*) par des variables (*explicatives, indépendantes*) via des équations (ou fonctions) mathématiques impliquant des **paramètres**.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Formule mathématiques d'une régression linéaire multiple :\n",
    "\n",
    "\\begin{equation}y=w_1x_1 + w_2x_2 + b\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "si les features $x_1$ et $x_2$ sont corrélées alors on peut écrire $x_2$ en fonction de $x_1$ :\n",
    "\n",
    "\\begin{equation}x_2=z_1x_1 + c\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "si on remplace $x_2$ par $z_1x_1 +c$ dans la formule de $y$ cela donne :\n",
    "\n",
    "\\begin{equation}\n",
    "   y=w_1x_1 + w_2(z_1x_1 + c) + b \n",
    "    \\iff y=(w_1+w_2z_1)x_1 + (b+w_2c)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Comment détecter la multicolinéarité des features :\n",
    "\n",
    "- **La tolérance** : elle est calculée en prenant en compte les variables déjà utilisées dans le modèle.\n",
    "\n",
    "La tolérance vaut (1-R²). \n",
    "Elle est utilisée comme un critère de filtrage des variables. \n",
    "Si une variable a une tolérance inférieure à un seuil fixé , on ne la laisse pas entrer dans le modèle car sa contribution est négligeable et elle risquerait d'entraîner les problèmes numériques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- **Le VIF**. Le VIF ou Variance Inflation Factor qui est égal à l'inverse de la tolérance.\n",
    "\n",
    "L’indice VIF ou Variance Inflation Factor mesure le degré de redondance d’une variable explicative avec les autres variables explicatives.\n",
    "Si une variable a un VIF supérieur à un seuil fixé , on ne la laisse pas entrer dans le modèle car elle n'apporte pas d'informations supplémentaires sur la variable à expliquer.\n",
    "*exemple : arrêter de mesurer une variable sur une chaîne de fabrication car elle est fortement liée à d'autres qui sont aussi mesurées*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Conclusion :\n",
    "\n",
    "*Certaines variables très corrélées peuvent se masquer les unes les autres dans l’évaluation de leurs effets. Cela peut même empêcher le modèle de se calculer*\n",
    "\n",
    "*Lorsqu’une régression linéaire multiple est exécutée sur des variables explicatives multicolinéaires, les estimations des coefficients peuvent être incorrectes*\n",
    "\n",
    "*Garder des features corrélées entraîne une perte de performance du modèle de régression linéaire*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Idée :\n",
    "\n",
    "**prendre un dataframe petit (que j'ai créé ou reprendre Chocolat et enlever des features quali) et expliquer avec des graphs les problèmes engendrés par la corrélation des features**"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Diaporama",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
